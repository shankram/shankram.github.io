---
title: "DragGAN"
date: 2024-08-30
tags:
- Vision
---

DragGAN - Point-based Image Manipulation : Overview

Generative Adversarial Networks (GANs) have revolutionized image generation over the past several years. Models like StyleGAN2 enable the creation of high-quality images that closely resemble the features of the training dataset. While advancements in architecture and training paradigms have significantly improved image generation quality, progress in *manipulating* generated images has been limited. For example, while a conditional GAN can generate an image of a person with specific attributes, altering these attributes in the generated image remains challenging. DragGAN is a **point-based image manipulation** algorithm designed to address this problem.

[Drag Your GAN](https://arxiv.org/pdf/2305.10973) (or DragGAN) by Xingang Pan et al. allows users to choose any number of *handle* and *target* points on an image generated by a GAN, where the goal is to move the handle points to the target points (see the figure below for reference). This allows the user free control over the spatial attributes of the image. DragGAN achieves this through an iterative process which ensures 
- The handle points move to the target points gradually (*motion supervision*),
- The position of the handle points are known after each step (*point tracking*).

<figure>
<img src='https://www.genape.ai/wp-content/uploads/2023/10/%E8%9E%A2%E5%B9%95%E6%93%B7%E5%8F%96%E7%95%AB%E9%9D%A2-2023-10-12-160923.jpg' width = 800 alt='Figure that explains it' class='center'>
<figcaption>Handle points (red) are moved towards target points (blue) while retaining the textural features.</figcaption>
</figure>

Some details about StyleGAN2's architecture that DragGAN requires: 
1. **Mapping Network**: 8-layer MLP used to map a 512-dimensional vector $z \in \mathcal{N}(0,1)$ to a 512-dimensional *style vector* $w$.
2. **Generator**: Consists of $N$ identical blocks, each of which takes a copy of $w$ and the output of the previous block as input (first block takes a learned constant $4\times 4$ image; like a blank canvas!). Each block doubles the spatial dimension of the input. The generated image $I$ is of dimensions $2^{N-2}\times 2^{N-2}$.
3. **Style Mixing**: Two different style vectors $w_1, w_2$ are used in earlier and later blocks of the generator which enables the model to derive structural or spatial features (pose, facial expressions) from $w_1$ and textural features (color of fur) from $w_2$.

DragGAN is an optimization algorithm that modifies the style vector $w$ used to generate the image using motion supervision and point targeting. The handle points are moved towards the target points such that the structural features (pose of the lion, or eye of the cat in above figure) are edited while the textural features are *mostly* unchanged. Let's look at the process in more detail.

### Motion Supervision

The motion supervision step of the algorithm moves the handle points $p_i$ towards the respective target points $t_i$ using the following *motion supervision loss* to optimize the style vector $w$: 
$$ \mathcal{L} = \sum_{i=0}^{n} \sum_{q\in \mathbf{S_i}} \vert\vert\mathbf{F}(q+d_i) - \mathbf{F}(q)\vert\vert_1 + \lambda \vert\vert(\mathbf{F}-\mathbf{F_0})\vert\vert_1 $$
where $\mathbf{S_i}$ denotes the set of pixels within a distance of $r_i$ (hyperparameter) from $p_i$, $d_i$ denotes the unit vector from $p_i$ to $t_i$, $\mathbf{F}$ is the output (feature map) of the 6th block of StyleGAN2's generator bilinearly interpolated to the dimensions of the final image, and $\mathbf{F_0}$ is the feature map of the 6th block corresponding to the original image,

The first term in the loss ensures that the features in a small neighborhood of the handle points move toward the features of the target points in a slow and continuous fashion. The second term is a regularization term that ensures that the features of the rest of the image are not modified. When backpropagating, the gradient is *not* passed through $\mathbf{F}(q)$ since that might cause the model to move the target points towards the handle points. This can be done by using `torch.tensor.detach`.

### Point-tracking

Each iteration of the motion supervision step results in a new style vector $w'$, image $I'$ and feature map $\mathbf{F}'$. Therefore, the location of the originally chosen handle points are no longer the same. We update the location of the handle points using point tracking and repeat the algorithm till they converge to the target points. Point tracking is done by searching for points $p'_i$ with the closest features (in $\mathbf{F}'$) to $p_i$ in the original feature maps.

$$ p'_i = \underset{\mathbf{q} \in \mathbf{S}'}{\text{argmin}} ||\mathbf{F'}(q) - \mathbf{F_0}(p_i)||_1 $$

where $\mathbf{S}'$ denotes the set of pixels within a distance of $r'_i$ from $p_i$.

When running StyleGAN2-FFHQ on an RTX Titan, the algorithm takes approximately 100 seconds for two handle points to converge to their target points. However, the generated image may have artifacts and hallucinations if the attribute combination has not been seen by the model during the training phase. Applying the method proposed [here](https://arxiv.org/pdf/2004.02546) by Erik Harkonen et al. in conjunction with this algorithm shows promise in mitigating these problems. The authors propose using a mask $\mathbf{M}$ which is multiplied to the regularization term of motion supervision loss to ensure that the region outside the mask doesn't get modified by the model.  Real images can be inverted using GAN Inversion. The quality of inversion largely decides if the image can be manipulated effectively.

Here's the [code](https://github.com/shankram/DL-implementations/blob/main/DGAN.py) for you to try. Clone the [stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch) repository and modify `training/networks.py:Generator` to output the required feature maps along with the final image. 